kubeEtcd:
  enabled: false
kubeScheduler:
  enabled: false
kubeControllerManager:
  enabled: false
kubeProxy:
  enabled: false
coreDns:
  enabled: false
alertmanager:
  config:
    #global:
    #  smtp_from: youremail@gmail.com
    #  smtp_smarthost: smtp.gmail.com:587
    #  smtp_auth_username: youremail@gmail.com
    #  smtp_auth_password: 
    #  smtp_auth_identity: youremail@gmail.com
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 1h
      # receiver: email
      routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
        - match:
            alertname: CPUThrottlingHigh
          receiver: 'null'
        - match:
            alertname: KubeMemoryOvercommit
          receiver: 'null'
        - match:
            alertname: KubeCPUOvercommit
          receiver: 'null'
        - match:
            alertname: KubeletTooManyPods
          receiver: 'null'
        # The Chrony NTP daemon on Oracle Linux does not work with this metric
        - match:
            alertname: NodeClockNotSynchronising
          receiver: 'null'
    receivers:
      - name: 'null'
    # - name: email
    #   email_configs:
    #   - send_resolved: true
    #     to: youremail@gmail.com

    # Inhibition rules allow to mute a set of alerts given that another alert is firing.
    # We use this to mute any warning-level notifications if the same alert is already critical.
    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        # Apply inhibition if the alertname is the same.
        equal: ['alertname', 'namespace']
  alertmanagerSpec:
    replicas: 1
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi
    resources:
      limits:
        cpu: "1"
        memory: 64Mi
      requests:
        cpu: 5m
        memory: 16Mi
prometheus:
  prometheusSpec:
    retention: 3d
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    resources:
      limits:
        cpu: "2"
        memory: 4Gi
      requests:
        cpu: 100m
        memory: 2Gi
  service:
    sessionAffinity: "ClientIP"
prometheusOperator:
  resources:
    limits:
      cpu: "1"
      memory: 256Mi
    requests:
      cpu: 5m
      memory: 64Mi
prometheus-node-exporter:
  resources:
    limits:
      cpu: 500m
      memory: 50Mi
    requests:
      cpu: 5m
      memory: 16Mi
kube-state-metrics:
  resources:
    limits:
      cpu: "1"
      memory: 256Mi
    requests:
      cpu: 5m
      memory: 32Mi
grafana:
  plugins:
    - grafana-piechart-panel
  resources:
    limits:
      cpu: 500m
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 64Mi
  sidecar:
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 5m
        memory: 64Mi
  additionalDataSources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki:3100
      version: 1

additionalPrometheusRules:
  - name: additional-prometheus-rules
    groups:
      - name: kubernetes-node-cpu
        rules:
        - alert: NodeHighCPU
          annotations:
            message: "CPU of {{ $labels.instance }} has been above 90% for the past 10 minutes."
          expr: instance:node_cpu_utilisation:rate1m > 0.9
          for: 10m
          labels:
            severity: warning
            category: low
      - name: kubernetes-failing-pods
        rules:
        - alert: KubePodFailed
          annotations:
            message: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a Failing state for longer than 15 minutes."
          expr: sum by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kube-state-metrics",namespace=~".*",phase=~"Failed"}) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"}))) > 0
          for: 15m
          labels:
            severity: warning
      - name: kubernetes-pod-close-to-limit
        rules:
        - alert: KubePodCloseToLimit
          annotations:
            message: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is within 10% of its memory limit and could be OOMKilled."
          expr: avg(container_memory_working_set_bytes{container!="", container!="statsd-exporter", image!=""}) by (container,pod,namespace) / sum(kube_pod_container_resource_limits_memory_bytes{}) by (container,pod,namespace) > 0.9
          for: 1m
          labels:
            severity: warning
            category: low
      - name: kubernetes-node-high-memory
        rules:
        - alert: KubeNodeHighMemoryUsage
          expr: ((1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100) > 90
          for: 5m
          annotations:
            description: Memory usage on node {{ $labels.instance }} has been > 90% for the past 5m.
            summary: Node memory usage > 90% for the past 5m.
          labels:
            severity: warning
        - alert: KubeNodeHighMemoryUsage
          expr: ((1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100) > 95
          for: 5m
          annotations:
            description: Memory usage on node {{ $labels.instance }} has been > 95% for the past 5m.
            summary: Node memory usage > 95% for the past 5m.
          labels:
            severity: critical

